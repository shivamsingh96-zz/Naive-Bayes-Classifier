{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Naive Bayes Classifier For Classifying Whether The Tumor Is Benign or Malignant**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "77a59b094ff9a1a7a31a4e8c51921941f8ce383f"
   },
   "source": [
    "**What is Naive Bayes algorithm?**\n",
    "\n",
    "Naive Bayes is a classification technique based on Bayes’ Theorem(*Probability theory*) with an assumption that all the features that predicts the target value are independent of each other. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature in determining the target value.\n",
    "\n",
    "> Naive Bayes model is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods.\n",
    "\n",
    "Bayes theorem provides a way of calculating posterior probability P(c|x) - *(read as Probability of **c** given **x**)*,  from P(c), P(x) and P(x|c). Look at the equation below:\n",
    ">\n",
    "> $$\\mathbf{P} \\left({x \\mid c} \\right) = \\frac{\\mathbf{P} \\left ({c \\mid x} \\right) \\mathbf{P} \\left({c} \\right)}{\\mathbf{P} \\left( {x} \\right)}$$\n",
    "\n",
    "where,\n",
    "\n",
    "* *x is set of features*\n",
    "* *c is set of classes*\n",
    "* P(c|x) is the posterior probability of class (c, target) given predictor (x, attributes).\n",
    "* P(c) is the prior probability of class **c**.\n",
    "* P(x|c) is the observation density or likelihood which is the probability of predictor(the query  **x**) given class.\n",
    "* P(x) is the prior probability of predictor **x**, and it is also called as Evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c0ac90e7494324245b5c8088281a784e03690ac9"
   },
   "source": [
    "**Why should we use Naive Bayes ?**\n",
    "\n",
    "* As stated above, It is **_easy_** to build and is particularly useful for **_very large data sets_**.\n",
    "* It is **extremely fast** for both training and prediction.\n",
    "* It provide straightforward probabilistic prediction.\n",
    "* It is often very easily interpretable.\n",
    "* It has very few (if any) tunable parameters.\n",
    "* It perform well in case of categorical input variables compared to numerical variable(s). For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random \n",
    "import scipy.stats as S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing tha Datasets\n",
    "\n",
    "Data = pd.read_csv(\"data.csv\")\n",
    "Data.dropna(axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "            ...             radius_worst  texture_worst  perimeter_worst  \\\n",
       "0           ...                    25.38          17.33           184.60   \n",
       "1           ...                    24.99          23.41           158.80   \n",
       "2           ...                    23.57          25.53           152.50   \n",
       "3           ...                    14.91          26.50            98.87   \n",
       "4           ...                    22.54          16.67           152.20   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis(PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is generally used for dimensionality reduction...\n",
    "In this data we are using PCA for finding important features which have more effect/weightage on finding posterior probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(Data):\n",
    "    Data_array = np.array(Data)\n",
    "\n",
    "    #______ Make Zero Mean Distribution_____\n",
    "\n",
    "    Means_value = np.mean(Data_array,axis=0)  #finding mean of each columns\n",
    "    Means_value = Means_value.reshape(1,Data_array.shape[1])\n",
    "\n",
    "\n",
    "    Centred_value = Data_array - Means_value    #Substracting respective mean with their respective columns values\n",
    "\n",
    "    \n",
    "    #___finding covariance matrix of zero mean distrubuted value____\n",
    "\n",
    "    Covariance_matrix = np.cov(Centred_value , rowvar=0)    \n",
    "    Covariance_matrix.shape\n",
    "    \n",
    "    #__finding eigen values and eigen vectors of covariance matrix\n",
    "    values,vectors = np.linalg.eig(Covariance_matrix)\n",
    "    values = values.reshape(1,len(values))\n",
    "\n",
    "    values_index = np.argsort(values)   #getting original index on the basis of sorted values\n",
    "    values_index = values_index[0]\n",
    "\n",
    "\n",
    "    values_index = (values_index[::-1])   # transform sorted_index to descn. order\n",
    "    \n",
    "    values = values[:,values_index]     #getting values which will be in descn. order\n",
    "    \n",
    "    \n",
    "    #_______finding cummulative sum for calculating weightage change____\n",
    "    weightage_of_features = np.cumsum(values)/np.sum(values)\n",
    "    \n",
    "    features_list_index=[]  #__list of important features index\n",
    "\n",
    "    for i in range(0,len(weightage_of_features)):\n",
    "        weightage_in_percent = weightage_of_features[i]*100\n",
    "\n",
    "        if weightage_in_percent <= 99.9:\n",
    "            features_list_index.append(values_index[i])\n",
    "\n",
    "    return(features_list_index)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "no_features = PCA(Data.iloc[:, 2:])\n",
    "print(no_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> By Applying PCA on our dataset, important features are 2+0, 2+1 i.e. 2, 3 index of our data which is radius_mean and texture_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Now split our Data into training and testing set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(Data, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "># Training of Model and Seperating By Class: { Benign, Malignant }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperarting data by class\n",
    "dataB = train[train['diagnosis'] == 'B']\n",
    "dataM = train[train['diagnosis'] == 'M']\n",
    "\n",
    "# This function returns the mean and covariance matrix of provided data\n",
    "def calculate_mean_covMat(data):\n",
    "    return data.iloc[:, 2:4].mean(), np.cov(data.iloc[:, 2:4],rowvar=0)\n",
    "\n",
    "# Calculating mean and covariance matrix of Benign\n",
    "BT_mean, BT_cov = calculate_mean_covMat(dataB)\n",
    "\n",
    "# Calculating mean and covariance matrix of Benign\n",
    "MT_mean, MT_cov = calculate_mean_covMat(dataM)\n",
    "\n",
    "# Calculating the P(B) and P(M) independently\n",
    "P_B = dataB.shape[0]/train.shape[0]\n",
    "P_M = dataM.shape[0]/train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go any further we should know **Posterior Conditional Probability** which is,\n",
    "\n",
    "$\\mathbf{P} \\left({x \\mid c} \\right) = \\mathbf{P} \\left ({c \\mid x} \\right) \\mathbf{P} \\left({c} \\right)$\n",
    "\n",
    "where, $\\mathbf{P} \\left ({c \\mid x} \\right)$ is ***Observation Distribution***\n",
    "\n",
    "And Mathematical Formula of Observation Distribution is\n",
    "\n",
    "$$ \\frac {1}{(\\sqrt{2}\\pi)^2\\sqrt{\\textstyle\\sum}}e^{-0.5}A^T{\\textstyle\\sum}^{-1}A $$\n",
    "\n",
    "where,\n",
    "\n",
    "* $ \\textstyle\\sum $    is a covariance matrix\n",
    "\n",
    "* A is a vector which contains \n",
    "$\n",
    "A=\n",
    "  \\left [ \n",
    "      {\\begin{array}{c}\n",
    "           R_i - Mean(radius\\_mean) \\\\\n",
    "           T_i - Mean(texture\\_mean) \\\\\n",
    "      \\end{array} } \n",
    "  \\right]\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "># Testing of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns the Observation Distribution\n",
    "def calculateObservationDistribution(test, mean, covMat):\n",
    "    return S.multivariate_normal.pdf(test, mean, covMat)\n",
    "\n",
    "\n",
    "# Here we are Calculating the Posterior Conditional Probability of Benign and Malignant Data\n",
    "PosteriorConditionalProbabilityB = calculateObservationDistribution(test.iloc[:, 2:4], BT_mean, BT_cov)*P_B\n",
    "PosteriorConditionalProbabilityM = calculateObservationDistribution(test.iloc[:, 2:4], MT_mean, MT_cov)*P_M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "># ***Prediction***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this section we are labelling whether it is Benign or Malignant\n",
    "\n",
    "# creating empty list of label prediction\n",
    "label_prediction = []\n",
    "\n",
    "# Comparing PosteriorConditionalProbability of Benign and Malignant\n",
    "for b, m in zip(range(len(PosteriorConditionalProbabilityB)), range(len(PosteriorConditionalProbabilityM))):\n",
    "    if(PosteriorConditionalProbabilityB[b] > PosteriorConditionalProbabilityM[m]):\n",
    "        label_prediction.append('B')\n",
    "    else:\n",
    "        label_prediction.append('M')\n",
    "\n",
    "# list to array\n",
    "label_prediction = np.array(label_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # ***Finding an Accuracy***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 91.22807017543859%\n"
     ]
    }
   ],
   "source": [
    "# Comapring all the rows of diagnosis of test data with label prediction\n",
    "count = 0\n",
    "total = len(test)\n",
    "for i in range(total):\n",
    "    if test.iloc[i, 1] == label_prediction[i]:\n",
    "        count += 1\n",
    "accuracy = count/total\n",
    "print('Accuracy = ' + str(accuracy*100) + '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
